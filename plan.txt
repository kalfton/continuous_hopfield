1. Current code can't successfully train the network. If it is not because of bug. Then One possibility is that the alg. works much better in [-1 1] case (try this hypo)
Also try my method 3, see if they the memory in discrete case is still stable in the continuous case.
2. Try for a trained network, assign it with different \tau with the same initial state, see if it will end up in different local minima.

Make the state represent g(x) rather than x

# It looks like the function: 1/(1+torch.exp(-beta*x)) somehow will make the gradient goes to zeros.
Solution: use torch.sigmoid() instead of self-defined function.


(12/6/2022)
A very interesting phenomena: Training Deep recurrence is worse than training shallow recurrence!!!!!

For shallow recurrence (1 step) Adam better than SGD, but for deep recurrence(>=2 steps) SGD better than Adam.

(12/7/2022)
Correction: Neither Adam or SGD successfully learn the patterns. They are mostly arrived at some gradient vanishing place and can't not get out after few updates.

(12/9/2022)
Adam looks much better than SGD to train the network.

Equilibium prop: When I set a large gamma(=100) the evolution can not converge to an stable point...

To do: convert all 3 methods into torch implementation!
print the loss in equilibium prop training!
Method 1 evaluate much slower, I can increase "dt" during training?

To do: set_W() function, and set_b() function. And make the alg.1 and alg.3's training algothms work on pytorch.

To do: (1). Compare the Weight matrix and bias between different training methods.(Done)
(2)Explore which alg. can train the network with more pattern stored.((PLA) \appox (back prop) > (eq prop))
(3) also, in method 1 and 2 is the weight always symmetric? (No, have made them symmetric now)

(4) Make the initiation of weight symmetric! (Done)
Theoritically, Method 1 and 3 should give symmetric weight, but not Method 2. How to solve it? (Done, just force the matrix to be symmetric when forward and backprop)

To do:
learn how to make the parallel computation code in python.

Back prop training method gives huge capacity than I expected... is this realistic? (Solved. In eq_prop and back_prop, the diagonal line is not restricted to be zero, thus make the pattern self sustanted.)

make the training alg. return the most accurate pattern that is stored in the network. (Done)

The back prop method seems to have some problem in converging? (done)
the forward function say it is converged, but the evolve function don't, why????(the min error set in backprop training is too high)
make the evolve alg. return the number of timestep took to retrieve the memory.(for retrieval time comparison.) (Done)

To do:
refactorizing the training code such that they process them in a batch?(done)
deprecate network.evolve() function.(done)

the energy function should also accommodate batch process(x)

By changing the init state slightly, the end state changes dramatically?
The PLA network  have much shorter retrieval time than the eq_prop network on the same init patterns, why?(Done, because W and b have different scale in the two networks.)

To do: change the init state slightly (from 1e-5 to 1e-4, see if it changes the results)
So there are states that is in the middle of 0 and 1 after retrieval.

Plan use ring init, and check if there is a init state retrieve too quickly(in that case, it is probably at a very flat place and didn't move at all.)

It looks like the tau set that have higher accuracy for memory retrieval also have faster speed to retrieve all memory?(Maybe?)

How to optimize {/tau} stochastically? (Kieferâ€“Wolfowitz algorithm for multidimension?)


In the proof of why assign small {\tau} to "hub" neuron is a good way: How to make ration with the maximal gradient descent strategy? (Done, local maximal gradient descend is not equal to global maximal gradient descend)

To do:
1. Try 50 neuron 30 patterns setting?
2. Try fminsearch in deterministic case.

